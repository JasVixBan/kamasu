\documentclass[9pt]{beamer}
\usepackage{wrapfig}
\usepackage{pgfpages}
\usepackage{listings}
\lstset{language=C++,
basicstyle=\ttfamily,
emph={const,typedef,struct,return,operator,float},       emphstyle=\color{orange},
emph={[2]bp,when,terminal,_value,_child0,_left,_right},  emphstyle={[2]\color{yellow}}
}

% \setbeameroption{show notes}
% \setbeameroption{show notes on second screen=bottom}

\mode<presentation>
{
  \usetheme{resophonic}
  \setbeamercovered{transparent}
%  \setbeamertemplate{alerted text begin}{hi hi hi}
%  \setbeamertemplate{alerted text end}{end end end}
}

\def\footertext#1{\hbox{\vbox to \logobarheight{
      \vfill\hbox{{\usebeamerfont{logobar}\usebeamercolor[fg]{logobar}#1}}\vfill}}}

% \def\rnote<#1>#2{\note<#1>{\whitesheets #2}}
% \newcommand{\rnote}[2]{ \note#1{#2}}

\addlogo{\footertext{Boostcon 2009}}
\logobartext{Troy D.~Straszheim}

\usepackage{multimedia}

\usepackage[english]{babel}

\usepackage[latin1]{inputenc}

\usepackage{times}
\usepackage[T1]{fontenc}

\def\fleh{\hskip0pt}
\def\py{>\fleh>\fleh>}
\def\launch#1#2{#1<\hskip0pt<\hskip0pt<#2>\hskip0pt>\hskip0pt>}

% \addlogo{blah}
% \def\logobarlogo{hi}

\title[resophonic::kamasu]{\texttt{resophonic::kamasu}}

\subtitle{Computing on the GPU with CUDA and boost::proto}

\author{Troy D. Straszheim}

\institute[Resophonic Systems, Inc.] % (optional, but mostly needed)
{
  Resophonic Systems, Inc.\\
  Washington, DC
}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date[BoostCon 2009] % (optional, should be abbreviation of conference name)
{BoostCon, 2009\\
Aspen, CO}
% - Either use conference name or its abbreviation.
% - Not really informative to the audience, more for people (including
%   yourself) who are reading the slides online

\subject{Computer Science}
% This is only inserted into the PDF information catalog. Can be left
% out. 



% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
\AtBeginSubsection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}

\begin{document}

\begin{frame}
   \note{honored to be here,

    First read modern c++ design in about 2002 or 3, wrote everything
    with policy classes for awhile, still get a thrill out of it

    love this kind of thing as it gives me an opportunity to look in
    to the minds of people that are smarter than I am, and not in
    real-time, and in ascii, so there's little danger that I'll need
    eyebleach

    last year at boostcon, hartmut was encouraging

    i thought great, opportunity to learn proto, look into eric
    niebler's mind

    CUDA was getting buzz

    opportunity to do both, and for the next hour or so I'm going to
    tell you what happened.  these are the proceeds thus far.

    haven't really discussed what I'm doing with anybody yet, so I
    expect you to see lots of things I'm overlooking, looking forward
    to what happens after this talk, lots of time for questions and
    discussion.

    Not a finished product.  We don't have special handling for
    triangular matrices or any of that. 
   }
   \titlepage
\end{frame}
%   }
% 
\begin{frame}{The Underpants Gnomes' Business Plan}
  \begin{columns}[c]
    \column{0.5\textwidth}
    \begin{enumerate}
    \item Collect Underpants
    \item ???
    \item Profit
    \end{enumerate}
    \column{0.5\textwidth}
    \pgfdeclareimage[height=1in]{x}{underpants_gnomes}
    \pgfuseimage{x}
    \end{columns}
\end{frame}

\begin{frame}{The Kamasu Business Plan}
  \begin{columns}[c]
    \column{0.5\textwidth}
    \begin{enumerate}
    \item Learn boost::proto
    \item Learn CUDA
    \item Combine
    \item ??? \only<2>{\alert<2>{<-  You are here}}
    \item Profit
    \end{enumerate}
    \column{0.5\textwidth}
    \pgfdeclareimage[height=1in]{x}{underpants_gnomes}
    \pgfuseimage{x}
    \end{columns}

  \note{The good news is that the problem-hungry amongst you are about to be well fed

    I haven't written a compiler for the GPU using only templates.

    By combining proto and CUDA I went for cheap laughs, so to speak,
    and I'm sure some of you will find this in bad taste.
}

\end{frame}

\begin{frame}{Outline}
  \tableofcontents
  % You might wish to add the option [pausesections]
\end{frame}

\section{CUDA}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{The GPU vs the CPU}
  \begin{center}
  \pgfdeclareimage[width=4in]{x}{gpuvscpu}
  \pgfuseimage{x}
  \end{center}

  \note{ gpu specialized for compute-intensive, highly parallel
    computation, which is what grahics is all about, so more of it is
    dedicated to doing math.

    it has a lot less sophisticated flow control

    a lot smaller cache

    the green bits are 
    
    presumably that's not to scale
  }
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{The GPU vs the CPU}
  \begin{center}
  \pgfdeclareimage[width=4in]{x}{Flops_1_L}
  \pgfuseimage{x}
  \end{center}
  (image courtesy of NVIDIA)
  \note{
    whoa
  }
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{The GPU}
  \begin{center}
  \pgfdeclareimage[height=3in]{x}{Device}
  \pgfuseimage{x}
  \end{center}

  \note{ This is Single Instruction Multi Thread, handles hundreds of
    threads running different programs.

    Broken up into Streaming Multiprocessors, seen here, which have
    eight Scalar Processor cores, an instruction unit, shard memory,
    some cache.  This thing creates and manages threads and implements
    \_\_syncthreads() which is for synchronization.

    Constant cache is fast and constant

    Texture cache is fast and constant and has some extra capabilities
    for filtering and whatnot.

    video card GTX 280 in my machine has 30 of these multiprocessors,
    8 cores on each for a total of 240 cores, clock is 1.35GHz

    Each multiprocessor is composed of eight processors, so that a multiprocessor is
    able to process the 32 threads of a warp in four clock cycles.

    There is an organizational scheme that they use involving warps,
    blocks and grids, not very important for the sake of this talk.  }

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Building software to run on the gpu}

nvcc and the like
functions that run on the host only, on the device only,
and callable from the host that run on the device
\end{frame}

\begin{frame}[fragile]{Serially adding a scalar to an array}
  \begin{semiverbatim}void add(float* data, unsigned size, float scalar)
\{
  for(unsigned i=0; i<size; i++) 
    data[i] += scalar;
\}
  \end{semiverbatim}
  \note{looking at how to add a scalar to a vector.

    this is the sequential C version we're all familiar with.
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Adding a scalar to an array in cuda-parallel}
  \begin{semiverbatim}\alert<2>{__global__} void
add(float *data, float scalar)
\{
  data[\alert<3>{threadIdx.x}] += scalar;
\}
\visible<4>{
int main() \{
  int N = 1024;
  float *arr = make_vector_on_gpu(N);
  
  add<\hskip0pt<\hskip0pt<1, N>\hskip0pt>\hskip0pt>(arr, 3.14159);
\}
}


\end{semiverbatim}

  \note{ This is a 'kernel' or a function that executes on the device,
    but is called by the host.

    the \_\_global\_\_ specifier makes it so.  this function is compiled
    by nvcc into code that runs on the gpu.

    this threadidx is a built in variable that the nvcc compiler puts
    into every kernel function.  it is actually has 3 dimensional
    structure, a 2 dimensional 'grid' of 'blocks', and a thread index
    within the block.

    Here's how we launch it.  First we somehow make an array of N
    elements out on the GPU, we'll get to that, then we use this magic
    anglebracket syntax that NVCC preprocesses into a bunch of
    function calls to an nvidia written library that fetches the
    compiled code for the kernel from wherever its been stored, ships
    it out to the video card, and executes it however many times is
    specified with this grid/block stuff.  From our perspective they
    run simultaneously, though they might be on different
    multiprocessors and whatever.  When threads have to communicate
    with each other or synchronize access to shared memory things get
    more complicated but that's all outside the scope of this talk.

  }
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{CUDA Memory management}
  \begin{semiverbatim}
cudaError_t cudaMalloc(void** devPtr, size_t count );
cudaError_t cudaFree(void* devPtr);

cudaError_t cudaMemcpy(void* dst, const void* src, 
                       size_t count, 
                       enum cudaMemcpyKind kind);

cudaMemcpyHostToHost
cudaMemcpyHostToDevice
cudaMemcpyDeviceToHost
cudaMemcpyDeviceToDevice

cudaError_t cudaMemset(void* devPtr, int value, 
                       size_t count);
  \end{semiverbatim}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{A holder class}
  \begin{semiverbatim}template <typename T>
class holder
\{
    T* devmem;
    std::size_T size_;

  public:

    holder();
    holder(std::size_t n);
    ~holder();
    boost::shared_ptr<holder> clone();
    void resize(std::size_t size);
    T* data() \{ return devmem; \}
    std::size_t size() \{ return size_; \}
\};
\end{semiverbatim}
\note{it isn't an array, it could be the underlying data used by an a ndimensional
  array}
\end{frame}

\begin{frame}[fragile]{holder<T> implementations}
\begin{semiverbatim}template <typename T>
holder<T>::holder(std::size_t s) : size_(s)
\{
   cudaMalloc(reinterpret_cast<void**>(&devmem), 
              size_ * sizeof(T));
\}

template <typename T>
holder<T>::~holder()
\{
  if (devmem)
    cudaFree(devmem);
\}
  \end{semiverbatim}
\end{frame}

\begin{frame}[fragile]{holder<T> implementations}
\begin{semiverbatim}template <typename T>
boost::shared_ptr<holder<T> >
holder<T>::clone()
\{
   boost::shared_ptr<holder> nh(new holder(size_));
   cudaMemcpy(devmem, nh->devmem,
              sizeof(T) * size_,
              cudaMemcpyDeviceToDevice);       
   return nh;
\}
  \end{semiverbatim}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{holder<T> implementations}
\begin{semiverbatim}template <typename T>
void
holder<T>::put(const T* hostmem, std::size_t s)
\{
  if (s != size_)
    resize(s);
  cudaMemcpy(devmem, hostmem, s * sizeof(T),
             cudaMemcpyHostToDevice);
\}

template <typename T>
void
holder<T>::get(T* hostmem)
\{
  cudaMemcpy(hostmem, devmem, size_ * sizeof(T),
             cudaMemcpyDeviceToHost);
\}\end{semiverbatim}

\note{now we just pass these things around at the end of shared
  pointers and we're good when for instance one array is a slice
  of another array}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{numpy arrays}
\begin{semiverbatim}\py a
array([[  1.,   \alert<4>{2.},   3.,   4.,   5.],
\alert<2>{       [  6.,   \alert<5-6>{\alert<4>{7.},   8.,   9.,}  10.],}
       [ 11.,  \alert<5-6>{\alert<4>{12.},  13.,  14.,}  15.],
\alert<3>{       [ 16.,  \alert<4>{17.},  18.,  19.,  20.]}])

\only<2-4>{
\py a[1,:]
\alert<2>{array([  6.,   7.,   8.,   9.,  10.])}

\py a[3,::-1]
\alert<3>{array([ 20.,  19.,  18.,  17.,  16.])}

\py a[:,1]
\alert<4>{array([  2.,   7.,  12.,  17.])}
}\only<5->{
\py a[1:3, 1:4]
array([\alert<5>{[  7.,   8.,   9.],
       [ 12.,  13.,  14.]}])

\py a[2:0:-1, 3:0:-1]
array([\alert<6>{[  9.,   8.,   7.],
       [ 14.,  13.,  12.]}])

}
\end{semiverbatim}
\note{the python bindings for kamasu support exactly this syntax, there is a 
special function called when you use this slicing notation}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Kamasu arrays}
\begin{semiverbatim}
using resophonic::kamasu::_;

array<float> a(m,n), c(m,n,o,p,q); 

array<float> b = a(index_range(_,_), index_range(2));

b = a(index_range(_,_,-1), index_range(_,_,-1));

float f = b(0,0); // a(9,9)
\end{semiverbatim}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Kamasu array metadata}
\begin{semiverbatim}
template <typename T>
struct array_impl
\{
  std::size_t \alert<2>{dims}[KAMASU_MAX_ARRAY_DIM];
  std::size_t \alert<3>{factors}[KAMASU_MAX_ARRAY_DIM];
  int strides[KAMASU_MAX_ARRAY_DIM];

  offset_t offset;
  std::size_t linear_size;
  unsigned nd;

  shared_ptr<holder<T> > gpu_data;
\};

array a(10,10), b;
b = a.slice(index_range(_,_,-1), index_range(2));
b(4) // which index is that?

\end{semiverbatim}
\note{arrays can be views of the same data,

semantics is always 'shared' unless you specifically copy something... same as numpy.

gpu\_data is stored in column-major form for compatibilty with cublas,
(if the stride of the last dimension is one)

We need to be able to find the index in memory of a particular entry
if we index it with parenthesis, on the c++ side, and also, from the
CUDA side, we start with the 3-dimensional grid-block-thread index,
the indexes and dimensions of which have nothing to do with the array,
and we have to know which entry we're working on.

linear size could be recomputed, it is just the product of the dimensions, 
we precompute to save time, as numpy does

}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{boost::proto}
\subsection{transforms}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Life before proto}
\begin{semiverbatim}
array<float> operator+(const array<float>& a, float f);

array<float> a(10), b(10);
a = b + 14;
\end{semiverbatim}
\note{T::T()
T::T()
T::T(float)
T\& T::operator=(T\&\&)
a.value=14

important thing is that the rhs gets eagerly evaluated... the T on the
lhs has only operator=(T\&)

problem is when you try to make that operator+ return a template that
you can evaluate later}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{\alert<4-7>{b} \alert<3>{+} \alert<8-11>{7.0f};}
\begin{semiverbatim}
\alert<2>{expr<
  \alert<3>{tag::plus}, 
  list2<
    \alert<4>{expr<
      \alert<5>{tag::terminal}, 
      \alert<6>{array const&}, 
    >},
    \alert<7>{expr<
      \alert<8>{tag::terminal}, 
      \alert<9>{float const&}, 
    >} 
  >,
>} 
\end{semiverbatim}
\note{ 

  evaluates to this big nested type.

  it has a tag plus, which we'll use later when we want to actually do something
  
  plus has two children, the first is also an expression

  so expressions have tags, like plus, multiplies, comma, or terminal.
  everything is overloaded, that's all done for you.  

  When you see things laid out the way proto lays them out for you it is
  easier to think.  imv.
  
  
% boost::proto::exprns_::expr<boost::proto::tag::plus, boost::proto::argsns_::list2<boost::proto::exprns_::expr<boost::proto::tag::terminal, boost::proto::argsns_::term<bing::matrix&>, 0l>, boost::proto::exprns_::expr<boost::proto::tag::terminal, boost::proto::argsns_::term<int const&>, 0l> >, 2l>

% resophonic::kamasu::Expression<boost::proto::exprns_::expr<boost::proto::tag::plus, boost::proto::argsns_::list2<resophonic::kamasu::array<float>&, resophonic::kamasu::Expression<boost::proto::exprns_::expr<boost::proto::tag::terminal, boost::proto::argsns_::term<int const&>, 0l> > >, 2l> >
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{\visible<1-6>{\alert<6>{b} + 7.0f;}}
\begin{semiverbatim}
\only<1>{boost::proto::}\only<1-2>{exprns_::}expr<
   \only<1>{boost::proto::}tag::plus, 
   \only<1>{boost::proto::}\only<1-2>{argsns_::}list2<
     \only<1>{boost::proto::}\only<1-2>{exprns_::}expr<
       \only<1>{boost::proto::}tag::terminal, 
       \only<1>{boost::proto::}\only<1-2>{argsns_::}\alert<4>{term}\hskip0pt<\alert<6>{array}>, 
       \alert<5>{0}
     >, 
     \only<1>{boost::proto::}\only<1-2>{exprns_::}expr<
       \only<1>{boost::proto::}tag::terminal, 
       \only<1>{boost::proto::}\only<1-2>{argsns_::}\alert<4>{term}\hskip0pt<int const&>, 
       \alert<5>{0}
     > 
   >, 
   \alert<5>{2}
>
\end{semiverbatim}
\note{ 
  actually its more complicated than that

  so the first thing we'll look at is where did this expression come
  from, if this object 'b' is just of type array?  Actually it came
  from someplace irrelevant, but we'll look at how to make this array
  type a proto expression itself.

}
\end{frame}

  % FIXME can't explain this
%  So we have two elementwise additions and a multiplication (which
%  isn't elementwise).  Still haven't thought about things like
%  elementwise multiplication vs matrix multiplication, or how useful
%  they are.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{an array is an expression}
\begin{semiverbatim}
proto::exprns_::\alert<2>{expr}\hskip0pt<
  proto::\alert<3>{tag::terminal}, 
  proto::argsns_::term<\alert<4>{T}>, 
  0
>

\alert<6>{\alert<5>{proto::terminal}\hskip0pt<T>::type}

namespace bp = boost::proto;

struct array_impl \{ /* data goes here */ \};

struct array : proto::terminal<array_impl>::type
\{ ... \};

\end{semiverbatim}
\note{ 

  the overloads that build the expression are defined by proto,
  for this type 'expr' which holds a terminal of some type T

  proto has lots of metafunctions, and this one terminal, luckily,
  will create that expression type for us.

  the array\_impl is also a good place for a compiler firewall since
  we're not doing everything in our headers

  so now this array type participates in all the overloads,
  transforms, domains, grammars and other goodies that proto provides.

}
\end{frame}

  % FIXME can't explain this
%  So we have two elementwise additions and a multiplication (which
%  isn't elementwise).  Still haven't thought about things like
%  elementwise multiplication vs matrix multiplication, or how useful
%  they are.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Hello World}
\begin{semiverbatim}struct array_impl \{ \};

struct array : bp::terminal<array_impl>::type
\{
  template <typename Expr>
  void operator=(const Expr& expr)
  \{
    std::cout << name_of(expr);
  \}
\};

array a, b;
a = b + 7.0f;

boost::proto::exprns_::expr<boost::proto::tag::plus, 
boost::proto::argsns_::list2<array&, boost::proto::e
xprns_::expr<boost::proto::tag::terminal, boost::pro
to::argsns_::term<float const&>, 0l> >, 2l>
\end{semiverbatim}
\note{ 

  the overloads that build the expression are defined by proto,
  for this type 'expr' which holds a terminal of some type T

  proto has lots of metafunctions, and this one terminal, luckily,
  will create that expression type for us.

  the array\_impl is also a good place for a compiler firewall since
  we're not doing everything in our headers

  so now this array type participates in all the overloads,
  transforms, domains, grammars and other goodies that proto provides.

}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{display\_expr}
\begin{semiverbatim}\alert<2>{std::ostream& operator<<(std::ostream& s, array_impl) 
\{
  return s << "array_impl";
\}}

struct array : bp::terminal<array_impl>::type
\{
  template <typename Expr>
  void operator=(const Expr& \alert<4>{expr})
  \{
    std::cout << \alert<3>{bp::display_expr}(\alert<4>{expr});
  \}
\};
\begin{columns}[t]
  \column{0.5\textwidth}\only<-6>{\alert<5>{array a, b;
a = b + 7.0f;


}}\column{0.5\textwidth}\only<6>{\alert<6>{plus(
    terminal(array_impl)
  , terminal(7)
)
}}
\end{columns}
\end{semiverbatim}
\note{ clearly display\_expr has somehow walked the expression tree,
  printing things as it goes along.  Let's do that.
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Overloaded operators}
\begin{semiverbatim}trespasser s, t;   array m, n;
m = \alert<4>{s} \alert<3>{&} \alert<5>{~n} \alert<2>{>>=} \alert<7>{std::cout} \alert<6>{%} \alert<9>{m}\alert<8>{->*}\alert<10>{t};

\alert<2>{shift_right_assign}(
    \alert<3>{bitwise_and}(
        \alert<4>{terminal(trespasser)}
      , \alert<5>{complement(
            terminal(array_impl)
        )}
    )
  , \alert<6>{modulus}(
        \alert<7>{terminal(0x607068)}
      , \alert<8>{mem_ptr}(
            \alert<9>{terminal(array_impl)}
          , \alert<10>{terminal(trespasser)}
        )
    )
)
\end{semiverbatim}
\note{
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Matching expressions}
\begin{semiverbatim}struct Grammar
  : bp::or_<bp::terminal<array_impl>,
            bp::terminal<float>,
            bp::plus<Grammar, Grammar>
            >
\{ \};

template <typename Expr>
void
array::operator=(const Expr& expr)
\{
  std::cout << bp::matches<Expr, Grammar>() << "\\n";
\}

m = s & ~m >\hskip0pt>= std::cout % n->*t;   // prints '0'
m = n + 7.0f;                       // prints '1'
\end{semiverbatim}
\note{ 
you can do this with enable\_if, which gives you an error that
looks like roadkill,

or with MPL\_ASSERT, which looks like not so much like roadkill but
like something that is just injured, with the stars around its head
there.  but at least should give the user the idea that the errror is
on purpose, library is trying to tell him something, not that he
hasn't found a bug }
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Detecting invalid expressions}
\begin{semiverbatim}template <typename Expr>
\only<1,4->{void}\only<2-3>{\alert<2>{typename enable_if<bp::matches<Expr, Grammar> >::type}}
array::operator=(const Expr& expr)
\{
  // to come: do something with expr
  \only<4->{\alert<4>{BOOST_MPL_ASSERT((bp::matches<Expr, Grammar>));}}
\}

m = s & ~m >\hskip0pt>= std::cout % n->*t;
\only<1-2,4>{






}\only<3>{\alert<3>{error: no match for 'operator=' in 'm = boost::prot
o::exprns_::operator>>= [with Left = boost::proto::
exprns_::expr<boost::proto::tag::bitwise_and, boost
::proto::argsns_::list2<boost::proto::exprns_::expr
<boost::proto::tag::terminal, boost::proto::argsns_
::term<trespasser&>, 0l>, const boost::proto::exprn
s_::expr<boost::proto::tag::complement, boost::prot
o::argsns_::list1<boost::proto::exprns_::expr<bo...}}\only<5-6>{error: no matching function for call to 'assertion_
failed(mpl_::failed\alert<6>{************ }boost::proto::resul
t_of::matches<boost::proto::exprns_::expr<boost::pr
oto::tag::shift_right_assign, boost::proto::argsns_
::list2<const boost::proto::exprns_::expr<boost::pr
oto::tag::bitwise_and, boost::proto::argsns_::list2
<boost::proto::exprns_::expr<boost::proto::tag::ter
mina  ....  proto::a>, 2l>, Grammar>::\alert<6>{************})'}
\end{semiverbatim}
\note{ 
you can do this with enable\_if, which gives you an error that
looks like roadkill,

or with MPL\_ASSERT, which looks like roadkill with stars in in.  not
so much like roadkill but like something that is just injured, with
the stars around its head there.  but at least should give the user
the idea that the errror is on purpose, library is trying to tell him
something, not that he hasn't found a bug }
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Transforms}
\begin{semiverbatim}
struct array_impl \{ float value; \};
struct array : bp::terminal<array_impl>::type
\{ /* ... */ \};

array \alert<2>{a};

\alert<3>{array_impl& aimpl =}
\alert<4>{   a.child0;}          // via base class' member
\alert<5>{   bp::value(a);}      // free function
   \alert<8>{\alert<7>{\alert<6>{bp::_value}()}(a);}   // instance of _value
   \alert<9>{bp::_child0()(a);}  // instance of _child0
   \alert<10>{bp::child_c<0>(a);}
   \alert<11>{bp::child<mpl::long_<0> >(a);}
   \alert<12>{bp::_left()(a);}    // instance of _left
   \alert<13>{bp::left(a);}       // left function
\end{semiverbatim}
\note{ \_value the type is different than value the free function for a
  very important reason and spectacularly nifty trick, we'll see in a
  minute.

  so what is the return type of the function bp::value?  it is the
  result of running the transform \_value on the object.  How is that
  determined?  By types nested inside this \_value transform type.
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{More transforms}
\begin{semiverbatim}
struct array_impl \{ float value; \};
struct array : bp::terminal<array_impl>::type \{ ... \};
array t;

\alert<4>{\only<4-5>{bp::display_expr(}\alert<2>{t + 2}\only<4-5>{)}}
\begin{columns}[t]
  \column{0.5\textwidth}\alert<3>{expr<
  tag::plus 
, list2<
    array&
  , expr<
      tag::terminal 
    , term<int const&> 
    , 0
    > 
  >
, 2
>}
\column{0.5\textwidth}\alert<5>{plus(
    terminal(array_impl)
  , terminal(2)
)}
\end{columns}



\end{semiverbatim}
\note{ }
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{More transforms}
\begin{semiverbatim}
struct array_impl \{ float value; \};
struct array : bp::terminal<array_impl>::type \{ ... \};
array t;

\only<1>{bp::display_expr(t + 2)
}\only<2>{bp::display_expr(bp::child1(t + 2))
}\only<3>{std::cout << bp::value(bp::child1(t + 2))}


\only<1>{plus(
    terminal(array_impl)
  , terminal(2)
)}\only<2>{terminal(2)


}\only<3>{2



}









\end{semiverbatim}
\note{ }
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{More transforms}
\begin{semiverbatim}
struct array_impl \{ float value; \};
struct array : bp::terminal<array_impl>::type \{ ... \};
array t;

\alert<8>{bp::value(\alert<6>{bp::right(\alert<4>{bp::left(\alert<2>{\alert<5>{a++->* \alert<7,9>{7}} /= 4})})});}

\alert<3>{divides_assign(
\alert<5>{    mem_ptr(
        post_inc(
            terminal(array_impl)
        )
      , \alert<7>{terminal(\alert<9>{7})}
    )}
  , terminal(4)
)}
\end{semiverbatim}
\note{ 

but the free functions aren't what is interesting.  it is the
transforms, or function objects that are nice.
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{proto::when}
\begin{semiverbatim}
bp::terminal<float>::type f;
f.child0 = 3.14;

std::cout << bp::value(f);    // prints 3.14
std::cout << bp::_value()(f); // also prints 3.14

struct FloatTerminal
: bp::when<bp::terminal<float>, bp::_value>
\{ \};

FloatTerminal transform;
std::cout << transform(f);  // prints 3.14




\end{semiverbatim}
\note{ }
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{proto::when}
\begin{semiverbatim}
struct ToString : bp::callable
\{
  typedef std::string result_type;
  
  template <typename T>
  result_type operator()(const T& t)
  \{
    return str(boost::format("%s @ %p") % t % &t);
  \}
\};

struct FloatTerminal
  : bp::when<bp::terminal<float>, ToString(bp::_value)>
\{ \};

bp::terminal<float>::type f = {3.14}; 
std::cout << FloatTerminal()(f);  
``3.14 @ 0x7fff0d839aa0''

\end{semiverbatim}
\note{ }
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{proto::when}
\begin{semiverbatim}
struct \alert<3>{ArrayTerminal}
  : bp::when<\alert<2,8>{bp::terminal<\alert<9>{array_impl}>}, \alert<11>{ToString(bp::_value)}>
\{ \};

struct \alert<5>{FloatTerminal}
  : bp::when<\alert<4>{bp::terminal<float>}, ToString(bp::_value)>
\{ \};

struct Grammar : 
  bp::or_<ArrayTerminal,
          FloatTerminal,
          bp::when<\alert<6>{bp::plus<\only<1-2>{\alert<2>{bp::terminal<array_impl>}}\only<3->{\alert<3>{ArrayTerminal}}, 
                            \only<1-4>{\alert<4>{bp::terminal<float> >}}\only<5->{\alert<5>{FloatTerminal}}>},
                   \alert<7>{ToString(\alert<14>{bp::tag::plus()},
                            \only<-11>{\alert<11>{\alert<10>{ToString}(\alert<9>{bp::_value(\alert<8>{bp::_left})})}}\only<12->{\alert<12>{ArrayTerminal(bp::_left)}},
                            \only<-12>{ToString(bp::_value(bp::_right)))}}\only<13->{\alert<13>{FloatTerminal(bp::_right))}}>
	    >
\{ \};      \only<14>{\alert<14>{// struct plus \{ \};}}
\end{semiverbatim}
\note{ }
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{They recurse!}
\begin{semiverbatim}
struct \alert<2>{ArrayTerminal}
  : bp::when<bp::terminal<array_impl>, ToString(bp::_value)>
\{ \};

struct \alert<2>{FloatTerminal}
  : bp::when<bp::terminal<float>, ToString(bp::_value)>
\{ \};

struct \alert<4>{Grammar} :
  bp::or_<\alert<3>{ArrayTerminal},
          \alert<3>{FloatTerminal},
          bp::when<bp::plus<\only<-3>{\alert<2>{ArrayTerminal}}\only<4->{\alert<4>{Grammar}},
                            \only<-3>{\alert<2>{FloatTerminal}}\only<4->{\alert<4>{Grammar}}>,
                   \alert<5>{ToString(bp::tag::plus(),
                            \only<-3>{\alert<2>{ArrayTerminal}}\only<4->{\alert<4>{Grammar}}(bp::_left),
                            \only<-3>{\alert<2>{FloatTerminal}}\only<4->{\alert<4>{Grammar}}(bp::_right))}>
     >
\{ \};
\end{semiverbatim}
\note{since plus is commutative, it doesnt matter what order the operands are in}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Tune up the transform}
\begin{semiverbatim}
ToString(bp::tag::plus(), 
         Grammar(bp::_left), Grammar(bp::_right))

struct ToString : bp::callable
\{
  typedef std::string result_type;

  template <typename T>
  result_type operator()(const T& t)
  \{
    return str(boost::format("%s @ %p") % t % &t);
  \}

  result_type operator()(bp::tag::plus, 
                         const std::string& lhs, 
                         const std::string& rhs)
  \{
    return str(boost::format("%s PLUS %s") % lhs % rhs );
  \}
\};
\end{semiverbatim}
\note{since plus is commutative, it doesnt matter what order the operands are in}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Tune up the transform}
\begin{semiverbatim}
struct Grammar : bp::or_< ...
struct ToString : bp::callable \{ ...
struct array : bp::terminal<array_impl>::type ...

array a;

std::cout << Grammar()(a + 777);

``array_impl @ 0x7fffe0f3f11f PLUS 777 @ 0x7fffe0f3f10c''

\end{semiverbatim}
\note{since plus is commutative, it doesnt matter what order the operands are in}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Avoiding temporaries 1: emulating rvalues}
\begin{semiverbatim}
struct \alert<2>{CopyLValue} : bp::callable
\{
  typedef array_impl<float> result_type;

  result_type
  operator()(const array_impl<float>& a)
  \{
    return a.clone();
  \}
\};


struct RkArrayTerminal 
  : bp::when<bp::terminal<rk::array_impl<float> >, 
             CopyLValue(bp::_value)>
\{ \};


\end{semiverbatim}
\note{

}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Kamasu hello world}
\begin{semiverbatim}

array<float> a, b, c;

\alert<2>{c = (a / 3.0f) * (b / 7.0f);}
\only<-3>{\alert<3>{
struct Array
  : bp::when<bp::terminal<rk::array_impl<float> >, 
             bp::_value>
\{ \};

struct Scalar
  : bp::or_<bp::when<bp::terminal<float>, bp::_value>, ...>
\{ \};
}}\only<4->{\alert<4>{
struct Grammar 
  : bp::or_<bp::when<bp::divides<\alert<5>{Array, Scalar}>,
                     \alert<5>{ArrayScalarOp}(bp::tag::divides(),
                                   Array(bp::_left), 
                                   Scalar(bp::_right))>,
            bp::when<bp::multiplies<\alert<6>{Array, Array}>,
                     \alert<6>{ArrayArrayOp}(bp::tag::multiplies(),
                                  Array(bp::_left), 
                                  Array(bp::_right))> 
            >
\{ \};
}}

\end{semiverbatim}
\note{

}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{a / 7.0f:  Transform declaration}
\begin{semiverbatim}

struct ArrayScalarOp : bp::callable
\{
  typedef rk::array_impl<float> result_type;

  template <typename Op>
  result_type 
  operator()(Op, const rk::array_impl<float>& v, const float& f);
\};


\end{semiverbatim}
\note{
This is also a compliation firewall  
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{a / 7.0f; Transform implementation}
\begin{semiverbatim}template <>
ArrayScalarOp::result_type
ArrayScalarOp::operator()(boost::proto::tag::minus, 
                          const rk::array_impl<float>& rv, 
                          const float& scalar)
\{
  switch (rv.nd) \{
    case 1:  k_easd1(rv.data() + rv.offset, rv.linear_size, 
                    rv.factors, rv.strides, 
                    scalar); break;
    case 2:  k_easd2(rv.data() + rv.offset, rv.linear_size, 
                    rv.factors, rv.strides, 
                    scalar); break;
    //...
        default:
    throw dimensions_out_of_range(rv.nd);
  \}
  return rv;
\}
\end{semiverbatim}
\note{
we're on the other side of the compliation firewall
}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{a / 7.0f: calculate gridsize, call kernel}
\begin{semiverbatim}
void k_easd2(float* data, 
             std::size_t linear_size,
             const std::size_t* factors, 
             const int* strides,
             float scalar)
\{
  bd_t bd = gridsize(linear_size);

  k_easd2_knl<\hskip0pt<\hskip0pt<bd.first, bd.second, 0>\hskip0pt>\hskip0pt>
    (data,
     linear_size,
     factors[0],factors[1],
     strides[0],strides[1],
     scalar);
\}
\end{semiverbatim}
\note{ 
  things get unrolled for passing to a cuda kernel.  We can't
  pass pointers, note that the factors and strides are in host memory,
  and these kernel functions expect pods
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{a / 7.0f: kernel function}
\begin{semiverbatim}__global__ void
k_easd2_knl(float* data,
            std::size_t linear_size,
            const std::size_t factor0, const std::size_t factor1,
            const int stride0, const int stride1,
            float scalar)
\{
  if (INDEX() >= linear_size)
    return;

  unsigned actual_index = 
    INDEX()/factor1*stride1 
    +  unsigned(INDEX() % factor1)/factor0*stride0;

  data[actual_index] /= scalar;
\}
\end{semiverbatim}
\note{ 
  things get unrolled for passing to a cuda kernel.  We can't
  pass pointers, note that the factors and strides are in host memory,
  and these kernel functions expect pods
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]{}
% \begin{semiverbatim}template <>
% ArrayArrayOp::result_type
% ArrayArrayOp::operator()(boost::proto::tag::multiplies, 
%                          const rk::array_impl<float>& lhs, 
%                          const rk::array_impl<float>& rhs) 
% \{
%   // verify dimensions, etc.
% 
%   cublasSgemm(lhs, rhs) // call cublas matrix multiplication
% 
%   return rv;
% \}
% \end{semiverbatim}
% \note{ 
%   things get unrolled for passing to a cuda kernel.  We can't
%   pass pointers, note that the factors and strides are in host memory,
%   and these kernel functions expect pods
% }
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \begin{frame}[fragile]{Avoiding temporaries 2: use the LHS}
% \begin{semiverbatim}
% 
% rk::array<float> a(10, 10);
% 
% a = sin(a);
% 
% \end{semiverbatim}
% \note{
%   in practice the LHS 
% }
% \end{frame}
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Avoiding temporaries 2: reuse the LHS}
\begin{semiverbatim}
struct CopyLValue : bp::callable
\{
  typedef array_impl<float> result_type;

  result_type
  operator()(const array_impl<float>& a, data_t& data)
  \{
    if (data.tmp == &a) \{ data.tmp = 0; return a; \} 
    else                \{ return a.clone();       \}
  \}
\};

struct RkArrayTerminal 
  : bp::when<bp::terminal<rk::array_impl<float> >, 
             CopyLValue(bp::_value, bp::_data)>
\{ \};


\end{semiverbatim}
\note{
  in practice the LHS 
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related efforts}

\begin{frame}{CuPP}
  
\end{frame}

\begin{frame}{komrade}
  
\end{frame}


\begin{frame}[fragile]{PyCuda (Andreas Kl\"ockner)}
  \begin{semiverbatim}import pycuda.autoinit, pycuda.driver as drv, numpy

mod = drv.SourceModule("""
__global__ void multiply_them(float *dest, float *a, float *b)
\{
  const int i = threadIdx.x;
  dest[i] = a[i] * b[i];
\}
""")

multiply_them = mod.get_function("multiply_them")
a = numpy.random.randn(400).astype(numpy.float32)
b = numpy.random.randn(400).astype(numpy.float32)
dest = numpy.zeros_like(a)
multiply_them(
    drv.Out(dest), drv.In(a), drv.In(b),
    block=(400,1,1))
print dest-a*b
  \end{semiverbatim}


  \note{the pycuda approach uses cuda's jit engine.  

    you can use any
    of various templating engines to 'metaprogram'}

\end{frame}

\section{resophonic::kamasu}
\subsection{benchmarks}

\begin{frame}{matrix multiplication}
gpu runs at theoretical 933GFlops peak 4k -on-a-side matrixis 16M of
single precision floats 68billion multiplies, only takes a couple of
seconds a singlethreaded cpu implementation just sits there staring
and you get bored.

on the other hand, operations with low numerical intensity like
multiplying a vector by a scalar are slower, by say 10 to 100

can't really benchmark the worst case, to move one float out to the
video card, launch a kernel to multiply it, move it back... the
multiply takes however many clock cycles on the cpu, and the gpu case
could be held up by who-knows what.   

So any real benchmarking here is premature optimization, you see posts
mentionin various speedups on various problems, YMMV.  

\end{frame}
\begin{frame}{scalar multiplication}

this is guarnteed to be slower no matter what, as a copy across the
bus costs more than a multiply (?)

\end{frame}

\begin{frame}{A = B * C}
  \pgfdeclareimage[height=3in]{x}{mm_close} 
  \pgfuseimage{x} 

  \note{
    so breakeven is at about 500.  This includes transfers to/from
    the card.  You can't even make the video card sweat, the
    computations are over in a flash, the fan in the card goes puff,
    lightly
  }

\end{frame}

\begin{frame}{A = B * C}
  \pgfdeclareimage[height=3in]{x}{mm_full} 
  \pgfuseimage{x} 

  \note{
    
  }

\end{frame}

\begin{frame}{A = B * C}
  \pgfdeclareimage[height=3in]{x}{mm_kamasu_vs_boost} 
  \pgfuseimage{x} 

  \note{ 
    kamasu vs boost times.  This just to show that they're
    spiking in about the same place.
  }

\end{frame}

\end{document}


