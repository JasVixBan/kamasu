\documentclass[10pt]{beamer}
\usepackage{wrapfig}
\usepackage{pgfpages}

\setbeameroption{show notes}
\setbeameroption{show notes on second screen=bottom}

\mode<presentation>
{
  \usetheme{resophonic}
  \setbeamercovered{transparent}
%  \setbeamertemplate{alerted text begin}{hi hi hi}
%  \setbeamertemplate{alerted text end}{end end end}
}

\def\footertext#1{\hbox{\vbox to \logobarheight{
      \vfill\hbox{{\usebeamerfont{logobar}\usebeamercolor[fg]{logobar}#1}}\vfill}}}

% \def\rnote<#1>#2{\note<#1>{\whitesheets #2}}
% \newcommand{\rnote}[2]{ \note#1{#2}}

\addlogo{\footertext{Boostcon 2009}}
\logobartext{Troy D.~Straszheim}

\usepackage{multimedia}

\usepackage[english]{babel}

\usepackage[latin1]{inputenc}

\usepackage{times}
\usepackage[T1]{fontenc}

\def\fleh{\hskip0pt}
\def\py{>\fleh>\fleh>}
\def\launch#1#2{#1<\hskip0pt<\hskip0pt<#2>\hskip0pt>\hskip0pt>}

% \addlogo{blah}
% \def\logobarlogo{hi}

\title[resophonic::kamasu]{\texttt{resophonic::kamasu}}

\subtitle{Computing on the GPU with CUDA and boost::proto}

\author{Troy D. Straszheim}

\institute[Resophonic Systems, Inc.] % (optional, but mostly needed)
{
  Resophonic Systems, Inc.\\
  Washington, DC
}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date[BoostCon 2009] % (optional, should be abbreviation of conference name)
{BoostCon, 2009\\
Aspen, CO}
% - Either use conference name or its abbreviation.
% - Not really informative to the audience, more for people (including
%   yourself) who are reading the slides online

\subject{Computer Science}
% This is only inserted into the PDF information catalog. Can be left
% out. 



% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
\AtBeginSubsection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}

\begin{document}

\begin{frame}
   \note{honored to be here,

    First read modern c++ design in about 2002 or 3, wrote everything
    with policy classes for awhile, still get a thrill out of it

    love this kind of thing as it gives me an opportunity to look in
    to the minds of people that are smarter than I am, and not in
    real-time, and in ascii, so there's little danger that I'll need
    eyebleach

    last year at boostcon, hartmut was encouraging

    i thought great, opportunity to learn proto, look into eric
    niebler's mind

    CUDA was getting buzz

    opportunity to do both, and for the next hour or so I'm going to
    tell you what happened.  these are the proceeds thus far.

    haven't really discussed what I'm doing with anybody yet, so I
    expect you to see lots of things I'm overlooking, looking forward
    to what happens after this talk, lots of time for questions and
    discussion.

    Not a finished product.
   }
   \titlepage
\end{frame}
%   }
% 
\begin{frame}{The Underpants Gnomes' Business Plan}
  \begin{columns}[c]
    \column{0.5\textwidth}
    \begin{enumerate}
    \item Collect Underpants
    \item ???
    \item Profit
    \end{enumerate}
    \column{0.5\textwidth}
    \pgfdeclareimage[height=1in]{x}{underpants_gnomes}
    \pgfuseimage{x}
    \end{columns}
\end{frame}

\begin{frame}{The Kamasu Business Plan}
  \begin{columns}[c]
    \column{0.5\textwidth}
    \begin{enumerate}
    \item Learn boost::proto
    \item Learn CUDA
    \item Combine
    \item ??? \only<2>{\alert<2>{<-  You are here}}
    \item Profit
    \end{enumerate}
    \column{0.5\textwidth}
    \pgfdeclareimage[height=1in]{x}{underpants_gnomes}
    \pgfuseimage{x}
    \end{columns}

  \note{The good news is that the problem-hungry amongst you are about to be well fed}

\end{frame}

\begin{frame}{Outline}
  \tableofcontents
  % You might wish to add the option [pausesections]
\end{frame}

\section{CUDA}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{The GPU vs the CPU}
  \begin{center}
  \pgfdeclareimage[width=4in]{x}{gpuvscpu}
  \pgfuseimage{x}
  \end{center}

  \note{ gpu specialized for compute-intensive, highly parallel
    computation, which is what grahics is all about, so more of it is
    dedicated to doing math.

    it has a lot less sophisticated flow control

    a lot smaller cache

    the green bits are 
    
    presumably that's not to scale
  }
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{The GPU}
  \begin{center}
  \pgfdeclareimage[height=3in]{x}{Device}
  \pgfuseimage{x}
  \end{center}

  \note{ This is Single Instruction Multi Thread, handles hundreds of
    threads running different programs.

    Broken up into Streaming Multiprocessors, seen here, which have
    eight Scalar Processor cores, an instruction unit, shard memory,
    some cache.  This thing creates and manages threads and implements
    \_\_syncthreads() which is for synchronization.

    Constant cache is fast and constant

    Texture cache is fast and constant and has some extra capabilities
    for filtering and whatnot.

    video card GTX 280 in my machine has 30 of these multiprocessors,
    8 cores on each for a total of 240 cores, clock is 1.35GHz

    Each multiprocessor is composed of eight processors, so that a multiprocessor is
    able to process the 32 threads of a warp in four clock cycles.

    There is an organizational scheme that they use involving warps,
    blocks and grids, not very important for the sake of this talk.  }

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Building software to run on the gpu}

nvcc and the like
functions that run on the host only, on the device only,
and callable from the host that run on the device
\end{frame}

\begin{frame}[fragile]{Serially adding a scalar to an array}
  \begin{semiverbatim}void add(float* data, unsigned size, float scalar)
\{
  for(unsigned i=0; i<size; i++) 
    data[i] += scalar;
\}
  \end{semiverbatim}
  \note{looking at how to add a scalar to a vector.

    this is the sequential C version we're all familiar with.
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Adding a scalar to an array in cuda-parallel}
  \begin{semiverbatim}\alert<2>{__global__} void
add(float *data, float scalar)
\{
  data[\alert<3>{threadIdx.x}] += scalar;
\}
\visible<4>{
int main() \{
  int N = 1024;
  float *arr = make_vector_on_gpu(N);
  
  add<\hskip0pt<\hskip0pt<1, N>\hskip0pt>\hskip0pt>(arr, 3.14159);
\}
}


\end{semiverbatim}

  \note{ This is a 'kernel' or a function that executes on the device,
    but is called by the host.

    the \_\_global\_\_ specifier makes it so.  this function is compiled
    by nvcc into code that runs on the gpu.

    this threadidx is a built in variable that the nvcc compiler puts
    into every kernel function.  it is actually has 3 dimensional
    structure, a 2 dimensional 'grid' of 'blocks', and a thread index
    within the block.

    Here's how we launch it.  First we somehow make an array of N
    elements out on the GPU, we'll get to that, then we use this magic
    anglebracket syntax that NVCC preprocesses into a bunch of
    function calls to an nvidia written library that fetches the
    compiled code for the kernel from wherever its been stored, ships
    it out to the video card, and executes it however many times is
    specified with this grid/block stuff.  From our perspective they
    run simultaneously, though they might be on different
    multiprocessors and whatever.  When threads have to communicate
    with each other or synchronize access to shared memory things get
    more complicated but that's all outside the scope of this talk.

  }
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{CUDA Memory management}
  \begin{semiverbatim}
cudaError_t cudaMalloc(void** devPtr, size_t count );
cudaError_t cudaFree(void* devPtr);

cudaError_t cudaMemcpy(void* dst, const void* src, 
                       size_t count, 
                       enum cudaMemcpyKind kind);

cudaMemcpyHostToHost
cudaMemcpyHostToDevice
cudaMemcpyDeviceToHost
cudaMemcpyDeviceToDevice

cudaError_t cudaMemset(void* devPtr, int value, 
                       size_t count);
  \end{semiverbatim}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{A holder class}
  \begin{semiverbatim}template <typename T>
class holder
\{
    T* devmem;
    std::size_T size_;

  public:

    holder();
    holder(std::size_t n);
    ~holder();
    boost::shared_ptr<holder> clone();
    void resize(std::size_t size);
    T* data() \{ return devmem; \}
    std::size_t size() \{ return size_; \}
\};
\end{semiverbatim}
\note{it isn't an array, it could be the underlying data used by an a ndimensional
  array}
\end{frame}

\begin{frame}[fragile]{holder<T> implementations}
\begin{semiverbatim}template <typename T>
holder<T>::holder(std::size_t s) : size_(s)
\{
   cudaMalloc(reinterpret_cast<void**>(&devmem), 
              size_ * sizeof(T));
\}

template <typename T>
holder<T>::~holder()
\{
  if (devmem)
    cudaFree(devmem);
\}
  \end{semiverbatim}
\end{frame}

\begin{frame}[fragile]{holder<T> implementations}
\begin{semiverbatim}template <typename T>
boost::shared_ptr<holder<T> >
holder<T>::clone()
\{
   boost::shared_ptr<holder> nh(new holder(size_));
   cudaMemcpy(devmem, nh->devmem,
              sizeof(T) * size_,
              cudaMemcpyDeviceToDevice);       
   return nh;
\}
  \end{semiverbatim}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{holder<T> implementations}
\begin{semiverbatim}template <typename T>
void
holder<T>::put(const T* hostmem, std::size_t s)
\{
  if (s != size_)
    resize(s);
  cudaMemcpy(devmem, hostmem, s * sizeof(T),
             cudaMemcpyHostToDevice);
\}

template <typename T>
void
holder<T>::get(T* hostmem)
\{
  cudaMemcpy(hostmem, devmem, size_ * sizeof(T),
             cudaMemcpyDeviceToHost);
\}\end{semiverbatim}

\note{now we just pass these things around at the end of shared
  pointers and we're good when for instance one array is a slice
  of another array}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{numpy arrays}
\begin{semiverbatim}\py a
array([[  1.,   \alert<4>{2.},   3.,   4.,   5.],
\alert<2>{       [  6.,   \alert<5-6>{\alert<4>{7.},   8.,   9.,}  10.],}
       [ 11.,  \alert<5-6>{\alert<4>{12.},  13.,  14.,}  15.],
\alert<3>{       [ 16.,  \alert<4>{17.},  18.,  19.,  20.]}])

\only<2-4>{
\py a[1,:]
\alert<2>{array([  6.,   7.,   8.,   9.,  10.])}

\py a[3,::-1]
\alert<3>{array([ 20.,  19.,  18.,  17.,  16.])}

\py a[:,1]
\alert<4>{array([  2.,   7.,  12.,  17.])}
}\only<5->{
\py a[1:3, 1:4]
array([\alert<5>{[  7.,   8.,   9.],
       [ 12.,  13.,  14.]}])

\py a[2:0:-1, 3:0:-1]
array([\alert<6>{[  9.,   8.,   7.],
       [ 14.,  13.,  12.]}])

}
\end{semiverbatim}
\note{the python bindings for kamasu support exactly this syntax, there is a 
special function called when you use this slicing notation}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Kamasu arrays}
\begin{semiverbatim}
using resophonic::kamasu::_;

array<float> a(m,n), c(m,n,o,p,q); 

array<float> b = a(index_range(_,_), index_range(2));

b = a(index_range(_,_,-1), index_range(_,_,-1));

float f = b(0,0); // a(9,9)
\end{semiverbatim}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related efforts}

\begin{frame}[fragile]{PyCuda (Andreas Kl\"ockner)}
  \begin{semiverbatim}import pycuda.autoinit, pycuda.driver as drv, numpy

mod = drv.SourceModule("""
__global__ void multiply_them(float *dest, float *a, float *b)
\{
  const int i = threadIdx.x;
  dest[i] = a[i] * b[i];
\}
""")

multiply_them = mod.get_function("multiply_them")
a = numpy.random.randn(400).astype(numpy.float32)
b = numpy.random.randn(400).astype(numpy.float32)
dest = numpy.zeros_like(a)
multiply_them(
    drv.Out(dest), drv.In(a), drv.In(b),
    block=(400,1,1))
print dest-a*b
  \end{semiverbatim}


  \note{the pycuda approach uses cuda's jit engine.  

    you can use any
    of various templating engines to 'metaprogram'}

\end{frame}

\section{boost::proto}
\subsection{transforms}

\section{resophonic::kamasu}
\subsection{benchmarks}

\end{document}


